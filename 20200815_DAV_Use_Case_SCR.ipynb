{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAV Use Case\n",
    "# How to Work With Comprehensive Internal Model Data for Three Portfolios\n",
    "\n",
    "\n",
    "### Authors:\n",
    "- Christian Jonen\n",
    "- Tamino Meyhöfer \n",
    "- Zoran Nikolić\n",
    "\n",
    "Also involved in various stages of the development:\n",
    "- Felix Terhag (first prototype for neural networks)\n",
    "- Michelle Anell\n",
    "- Helen Schneider\n",
    "\n",
    "The main intention of this notebook is to demonstrate how the comprehensive internal model data which we provide together with the notebook can be loaded and transformed in order to train different machine learning models.\n",
    "\n",
    "The scope of the data exceeds anything currently accessible in the open domain. Furthermore, we provide more data than is usually available in the productive environments of the insurance companies, which should enable the actuarial data scientists to reliably test their regression models and draw comparisons among them.\n",
    "\n",
    "The code below for the training of both neural networks and polynomials serves as a minimal viable example. The resulting neural network and regression polynomial are by no means optimized. They represent simple examples, in the case of the polynomial regression we use as terms only the linear factors of the risk factors. \n",
    "\n",
    "The users of this Use Case are encouraged to test other hyperparameter configurations and compare the results they obtain when doing so. Furthermore, a sophisticated hyperparameter optimization together with an ensemble technique will lead to a more stable and robust result. We believe that other regression techniques ranging from more traditional to the decision tree based approaches can equally well be applied to our data.\n",
    "\n",
    "The authors have implemented one such optimization. In the article we plan to publish soon after releasing the data and in our talks at the conferences of the German Actuarial Association we report the goodness-of-fit we have reached and compare it to the best polynomials we have fitted using the least-squares regression and an adaptive step-wise algorithm as described in Krah, Nikolić, Korn: \"A Least-Squares Monte Carlo Framework in Proxy Modeling of Life Insurance Companies\" (2018). These results should serve as a challenge to the actuarial/data science community. We are looking forward to approaches which will produce even better results. \n",
    "\n",
    "\n",
    "## Neural Network\n",
    "\n",
    "In this chapter we demostrate how our data can be used to train a neural network. For the given training data a neural network model is trained with hyperparameters included in the file setup_NN.csv. As noted above, various hyperparameter optimizations as well as ensemble methods can be implemented. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.layers import Input, Dense, LeakyReLU, Dropout\n",
    "from keras.models import Model, load_model\n",
    "import keras.optimizers as kOpt\n",
    "from keras.regularizers import l1_l2\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data and scale it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this portion of the code data are loaded for the neural network training. \n",
    "\n",
    "As explained in the accompanying description on the web page of the German Actuarial Association (in German) for all three portfolios we provide four sets of input and output data. Additionally, the standard errors for validation and nested data sets are provided. In the article we plan to publish later in 2020 we intend to describe the data in more detail in English. Our data consists of:\n",
    "\n",
    "- train_input.csv\n",
    "- train_result.csv\n",
    "- validation_input.csv\n",
    "- validation_result.csv\n",
    "- nested_input.csv\n",
    "- nested_result.csv\n",
    "- base_input.csv\n",
    "- base_result.csv    \n",
    "    \n",
    "The base case data are provided for the sake of completeness. This file does not include any new information, since our data is scaled in such a manner that the base case (all risk factors take value 0)  becomes 1. Technically, input files have to contain columns corresponding to input parameters and rows corresponding to samples. The first column and the first row are disregarded as they are considered headers. Output files have one column with header 'o1'. This column represents the scaled and transformed value of the economic own funds for each of the three portfolios.\n",
    "\n",
    "The following code reads in and scales the inputs and outputs for the neural network and automatically deletes all columns containing only nans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path):\n",
    "\n",
    "    # assert path is a string and the required files do exist\n",
    "    assert type(path)==str, \"path has to be a string\"\n",
    "    if not path[-1]=='/':\n",
    "        path=path+'/'\n",
    "    \n",
    "    assert os.path.exists(path+'train_input.csv'), \"File '\"+ path+\"train_input.csv' is missing\"\n",
    "    assert os.path.exists(path+'validation_input.csv'), \"File '\"+ path+\"validation_input.csv' is missing\"\n",
    "    assert os.path.exists(path+'nested_input.csv'), \"File '\"+ path+\"nested_input.csv' is missing\"\n",
    "    assert os.path.exists(path+'base_input.csv'), \"File '\"+ path+\"base_input.csv' is missing\"\n",
    "    assert os.path.exists(path+'train_result.csv'), \"File '\"+ path+\"train_result.csv' is missing\"\n",
    "    assert os.path.exists(path+'validation_result.csv'), \"File '\"+ path+\"validation_result.csv' is missing\"\n",
    "    assert os.path.exists(path+'nested_result.csv'), \"File '\"+ path+\"nested_result.csv' is missing\"\n",
    "    assert os.path.exists(path+'base_result.csv'), \"File '\"+ path+\"base_result.csv' is missing\"\n",
    "    \n",
    "        \n",
    "    # =============================================================================\n",
    "    #     Specify paths\n",
    "    # =============================================================================\n",
    "        \n",
    "    # train paths\n",
    "    path_train_res=path+'train_result.csv'\n",
    "    path_train_input=path+'train_input.csv'       \n",
    "            \n",
    "    # validation paths  \n",
    "    path_val_res=path+'validation_result.csv'\n",
    "    path_val_input=path+'validation_input.csv'  \n",
    "\n",
    "    # nested paths\n",
    "    path_nest_res=path+'nested_result.csv'\n",
    "    path_nest_input=path+'nested_input.csv'\n",
    "        \n",
    "    # base paths\n",
    "    path_base_res=path+'base_result.csv'\n",
    "    path_base_input=path+'base_input.csv'\n",
    "\n",
    "    \n",
    "    # =============================================================================\n",
    "    #   Load data\n",
    "    # =============================================================================\n",
    "            \n",
    "    # train set\n",
    "    train_results=pd.read_csv(path_train_res,index_col=0)\n",
    "    train_inputs=pd.read_csv(path_train_input,index_col=0)\n",
    "    \n",
    "    # validation set\n",
    "    val_results=pd.read_csv(path_val_res,index_col=0)\n",
    "    val_inputs=pd.read_csv(path_val_input,index_col=0)\n",
    "    \n",
    "    # nested set\n",
    "    nest_results=pd.read_csv(path_nest_res,index_col=0)\n",
    "    nest_inputs=pd.read_csv(path_nest_input,index_col=0)\n",
    "    \n",
    "    # base set\n",
    "    base_results=pd.read_csv(path_base_res,index_col=0)\n",
    "    base_results=base_results.iloc[[0],:]\n",
    "    base_inputs=pd.read_csv(path_base_input,index_col=0)\n",
    "\n",
    "    \n",
    "    # =============================================================================\n",
    "    #   Scale data    \n",
    "    # =============================================================================\n",
    "    \n",
    "    # calculate scaling\n",
    "    results_scale=pd.DataFrame(columns=train_results.columns)\n",
    "    inputs_scale=pd.DataFrame(columns=train_inputs.columns)\n",
    "    \n",
    "    results_scale.loc['min']=train_results.min()\n",
    "    results_scale.loc['max']=train_results.max()\n",
    "    \n",
    "    inputs_scale.loc['min']=train_inputs.min()\n",
    "    inputs_scale.loc['max']=train_inputs.max()\n",
    "    \n",
    "    train_results=(train_results-results_scale.loc['min',:])/(results_scale.loc['max',:]-results_scale.loc['min',:])\n",
    "    train_inputs=(train_inputs-inputs_scale.loc['min',:])/(inputs_scale.loc['max',:]-inputs_scale.loc['min',:])\n",
    "    \n",
    "    nest_results=(nest_results-results_scale.loc['min',:])/(results_scale.loc['max',:]-results_scale.loc['min',:])\n",
    "    nest_inputs=(nest_inputs-inputs_scale.loc['min',:])/(inputs_scale.loc['max',:]-inputs_scale.loc['min',:])\n",
    "    \n",
    "    val_results=(val_results-results_scale.loc['min',:])/(results_scale.loc['max',:]-results_scale.loc['min',:])\n",
    "    val_inputs=(val_inputs-inputs_scale.loc['min',:])/(inputs_scale.loc['max',:]-inputs_scale.loc['min',:])\n",
    "    \n",
    "    base_results=(base_results-results_scale.loc['min',:])/(results_scale.loc['max',:]-results_scale.loc['min',:])\n",
    "    base_inputs=(base_inputs-inputs_scale.loc['min',:])/(inputs_scale.loc['max',:]-inputs_scale.loc['min',:])\n",
    "    \n",
    "    \n",
    "    # Align order\n",
    "    base_results=base_results[train_results.columns]\n",
    "    base_inputs=base_inputs[train_inputs.columns]\n",
    "    \n",
    "    nest_results=nest_results[train_results.columns]\n",
    "    nest_inputs=nest_inputs[train_inputs.columns]\n",
    "    \n",
    "    val_results=val_results[train_results.columns]\n",
    "    val_inputs=val_inputs[train_inputs.columns]\n",
    "    \n",
    "    \n",
    "    return train_inputs,train_results, \\\n",
    "            val_inputs, val_results, \\\n",
    "            nest_inputs, nest_results, \\\n",
    "            base_inputs, base_results, \\\n",
    "            inputs_scale, results_scale\n",
    "       \n",
    "\n",
    "def scale_back(y,scale):\n",
    "\n",
    "    y=y*(scale.loc['max']-scale.loc['min'])+scale.loc['min']\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we establish functions for setting up the model, training it and evaluating the predictions.\n",
    "\n",
    "Following this, we build a feedforward neural network corresponding to the given hyperparameter specifications and return a keras model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model(hidden_layers=[100,100,100,100,100],dropouts=[0.3,0.3,0.3,0.3,0.2],input_dim=15, \n",
    "                activation='LeakyReLU',loss='mean_squared_error',optimizer='adam',verbose=True,\n",
    "                LReluAlpha=0.3,l1reg=0.,l2reg=0.,initializer='uniform',output_activation='tanh'):\n",
    "\n",
    "    '''\n",
    "    hidden_layers [int]      List of dimensions of the hidden layers.\n",
    "    \n",
    "    dropouts [float]         List of dropouts in the specific hidden layer, has to be of the same size as hidden_layers.\n",
    "                            \n",
    "    input_dim [int]          Input dimension.\n",
    "                            \n",
    "    activation               Has to be an identifier for keras activation functions,\n",
    "                             LeakyReLU can be used as an additional identifier.\n",
    "                                \n",
    "    loss                     Loss used for keras.models.Model.compile.\n",
    "    \n",
    "    optimizer                Optimizer used for keras.models.Model.compile.\n",
    "    \n",
    "    verbose [boolean]        If False no suppresses, all outputs from this function.\n",
    "                            \n",
    "    LReluAlpha               Alpha for LeakyReLU activation. If another activation is used, the value is disregarded.\n",
    "                        \n",
    "    l1reg                    L1 regularization term.\n",
    "        \n",
    "    l2reg                    L2 regularization term.\n",
    "    \n",
    "    initializer              Keras initializer. \n",
    "    \n",
    "    output_activation [str]  Activation of the output layer.\n",
    "    '''\n",
    "\n",
    "    # assert hidden_layers and dropouts have the same length\n",
    "    assert len(hidden_layers)==len(dropouts),\"hidden_layers and dropouts have to be of the same length\"\n",
    "    for i in range(len(hidden_layers)):\n",
    "        assert type(hidden_layers[i])==int, \"hidden dimensions have to be of type int\"\n",
    "    \n",
    "    assert type(input_dim)==int, \"input_dim has to be of type int\"\n",
    "    inputs = Input(shape=(input_dim,))\n",
    "    \n",
    "    '''first layer processing the input data'''\n",
    "    if activation=='LeakyReLU':\n",
    "        x=Dense(hidden_layers[0],kernel_regularizer=l1_l2(l1=l1reg, l2=l2reg),bias_regularizer=l1_l2(l1=l1reg, l2=l2reg),kernel_initializer=initializer)(inputs)\n",
    "        x=LeakyReLU(alpha=LReluAlpha)(x)\n",
    "        x=Dropout(dropouts[0])(x)\n",
    "    else:\n",
    "        x=Dense(hidden_layers[0],activation=activation,kernel_regularizer=l1_l2(l1=l1reg, l2=l2reg),bias_regularizer=l1_l2(l1=l1reg, l2=l2reg),kernel_initializer=initializer)(inputs)\n",
    "        x=Dropout(dropouts[0])(x)\n",
    "        \n",
    "    '''hidden layers'''\n",
    "    for i in range(1,len(hidden_layers)):\n",
    "        if activation=='LeakyReLU':\n",
    "            x = Dense(hidden_layers[i],kernel_regularizer=l1_l2(l1=l1reg, l2=l2reg),bias_regularizer=l1_l2(l1=l1reg, l2=l2reg),kernel_initializer=initializer)(x)\n",
    "            x = LeakyReLU(alpha=LReluAlpha)(x)\n",
    "            x = Dropout(dropouts[i])(x)\n",
    "        else:\n",
    "            x = Dense(hidden_layers[i],activation=activation,kernel_regularizer=l1_l2(l1=l1reg, l2=l2reg),bias_regularizer=l1_l2(l1=l1reg, l2=l2reg),kernel_initializer=initializer)(x)\n",
    "            x = Dropout(dropouts[i])(x)\n",
    "            \n",
    "    '''output layer'''\n",
    "    predictions = Dense(1, activation=output_activation)(x)\n",
    "           \n",
    "    # This creates a model that includes the input layer and some dense layers\n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "    \n",
    "    model.compile(optimizer=optimizer,loss=loss,metrics=['mean_squared_error','mean_absolute_error'])\n",
    "    \n",
    "    # print if verbose\n",
    "    if verbose:\n",
    "        model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the important parts of our proposal for the training is to train the model on the training set but use the validation set for the early stopping. We therefore build upon the data paradigm with three types of data (training, validation, nested) used in the LSMC risk capital proxy modelling implemented in the industry. The following code saves the model and the configuration in path if a path is given.\n",
    "\n",
    "The next function is called when the neural network model is trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,x,y,x_val,y_val,path=None,batchsize=128,epochs=300,early_stopping=True,early_stopping_no=200,\n",
    "                results_scale=None,time_budget=np.inf, in_nest=None, in_hval=None, in_base=None,res_nest=None, \n",
    "                res_hval=None,res_base=None,val_metric='mse',result_param='o1',verbose=True):\n",
    "    \n",
    "    '''\n",
    "    model                Model to be trained.\n",
    "     \n",
    "    x,y                  Training set.\n",
    "    \n",
    "    x_val, y_val         Validation set.\n",
    "    \n",
    "    path                 Saves the best and the last model in path/savemodels,\n",
    "                         saves the training results in path, aborts if path already exists.\n",
    "    \n",
    "    batchsize [int]      Batchsize.\n",
    "    \n",
    "    epochs [int]         Maximum number of epochs.\n",
    "    \n",
    "    early_stopping [boolean]\n",
    "                         Stops early if True and no improvement for [early_stopping_no] epochs.\n",
    "    \n",
    "    early_stopping_no [int] \n",
    "                         Stops training if it has not improved on the validation set for this number of consecutive epochs.\n",
    "                    \n",
    "    results_scale        The values to scale the result back to the original values in order to obtain the correct MSE.             \n",
    "                    \n",
    "    time_budget [float]  Stops if this time in seconds is used up.\n",
    "    \n",
    "    val_metric [string]  A keyword in ['mse', 'mae'], determines which metric is used for validation and hence for\n",
    "                         early stopping.\n",
    "    '''\n",
    "    \n",
    "    x=x.values\n",
    "    x_val=x_val.values\n",
    "    \n",
    "    y=y[result_param].values\n",
    "    y_val=y_val[result_param].values\n",
    "    \n",
    "    # check if all sets for full evaluation of all metrics are given\n",
    "    if in_nest is None:\n",
    "        full_eval_bool=False\n",
    "    elif in_hval is None:\n",
    "        full_eval_bool=False\n",
    "    elif in_base is None:\n",
    "        full_eval_bool=False\n",
    "    elif res_nest is None:\n",
    "        full_eval_bool=False\n",
    "    elif res_hval is None:\n",
    "        full_eval_bool=False\n",
    "    elif res_base is None:\n",
    "        full_eval_bool=False\n",
    "    else:\n",
    "        full_eval_bool=True\n",
    "        \n",
    "\n",
    "    print(\"%d Training samples\"%len(y))\n",
    "    print(\"%d Validation samples\"%len(y_val))\n",
    "\n",
    "    # loss on validation set\n",
    "    val_mse=[];\n",
    "    val_mae=[];\n",
    "    \n",
    "    # loss during training\n",
    "    train_mse=[];\n",
    "    train_mae=[];\n",
    "    \n",
    "    # factor to scale mse back to the mse on the original values\n",
    "    if not results_scale is None:\n",
    "        res_max=results_scale.loc['max',result_param]\n",
    "        res_min=results_scale.loc['min',result_param]\n",
    "        scale_factor=(res_max-res_min)**2\n",
    "        scale_factor_abs=res_max-res_min\n",
    "    else:\n",
    "        scale_factor=1;\n",
    "        scale_factor_abs=1;\n",
    "\n",
    "    # if path not None, check if path/savemodels exists    \n",
    "    if not path is None:\n",
    "        assert type(path)==str, \"Path has to be a string\"\n",
    "        assert not os.path.exists(path), \"Path '\"+ path+\"' already exists\"\n",
    "        if not path[-1]=='/':\n",
    "            path=path+'/'\n",
    "        # make folder\n",
    "        os.mkdir(path)\n",
    "        os.mkdir(path+'savemodels')\n",
    "        \n",
    "    \n",
    "    # save best epoch and best value for early stopping\n",
    "    best_epoch=0\n",
    "    best_value=np.inf\n",
    "    \n",
    "    start_t=time.time()\n",
    "    cur_t=time.time()\n",
    "    for epoch in range(epochs):\n",
    "        if verbose:\n",
    "            print(\"\\n==================Epoch %d==================\"%epoch)\n",
    "        # train one epoch # trains the model for a given number of epochs\n",
    "        hist=model.fit(x=x, y=y, validation_data=(x_val,y_val),batch_size=batchsize, epochs=epoch+1, verbose=0, callbacks=None, shuffle=True,  initial_epoch=epoch)\n",
    "        # append training loss\n",
    "        train_mse.append(scale_factor*hist.history['mean_squared_error'][-1])\n",
    "        train_mae.append(scale_factor_abs*hist.history['mean_absolute_error'][-1])\n",
    "        \n",
    "        mse=scale_factor*hist.history['val_mean_squared_error'][-1]\n",
    "        val_mse.append(scale_factor*hist.history['val_mean_squared_error'][-1])\n",
    "        val_mae.append(scale_factor_abs*hist.history['val_mean_absolute_error'][-1])\n",
    "        \n",
    "        # output current evaluations\n",
    "        if verbose:\n",
    "            print(\"Train MSE:\\t\\t%.5g\"%(scale_factor*hist.history['mean_squared_error'][-1]))\n",
    "            print(\"Val MSE:\\t\\t%.5g\"%(mse))\n",
    "        \n",
    "        \n",
    "        if val_metric=='mae':\n",
    "            current_best=scale_factor_abs*hist.history['val_mean_absolute_error'][-1]\n",
    "        else:\n",
    "            current_best=mse\n",
    "        \n",
    "        # check if current value is the best value encountered so far\n",
    "        if current_best<best_value:\n",
    "            best_value=current_best;\n",
    "            best_epoch=epoch\n",
    "            \n",
    "            # delete old best model if there is a model saved and save the current best\n",
    "            if not path is None:\n",
    "                if os.path.exists(path+'savemodels/model_best.h5'):\n",
    "                    os.remove(path+'savemodels/model_best.h5')\n",
    "                model.save(path+'savemodels/model_best.h5')\n",
    "\n",
    "        # check if it hasn't improved for early_stopping epochs\n",
    "        elif epoch-best_epoch>=early_stopping_no:\n",
    "            if early_stopping:  \n",
    "                print(\"\\nModel has not improved for %d consecutive Epochs\"%early_stopping_no)\n",
    "                print(\"Early Stopping...\")\n",
    "    \n",
    "                break;\n",
    "            \n",
    "        # break if time budget is used up\n",
    "        if time.time()-start_t>time_budget:\n",
    "            print(\"\\nStopped because time budget of %.1f is used up\"%time_budget)\n",
    "            print('Stopping...')\n",
    "            break;\n",
    "        if verbose:\n",
    "            print('Time: %.2fs'%(time.time()-cur_t))\n",
    "            print('Full Time: %.1fs'%(time.time()-start_t))\n",
    "        cur_t=time.time() \n",
    "        if verbose:\n",
    "            print(\"Current best Epoch:   %d\"%best_epoch)\n",
    "    \n",
    "    if epochs<epoch+1:\n",
    "        print(\"\\nMaximum number of epochs reached...\")\n",
    "    \n",
    "    # save training course\n",
    "    trainCourse={'mse':train_mse,'val_mse':val_mse,'mae':train_mae,'val_mae':val_mae}\n",
    "    if not path is None:\n",
    "        with open(path+'trainCourse.pkl','wb') as f:    \n",
    "            pickle.dump(trainCourse, f)                 \n",
    "        \n",
    "\n",
    "    # Output best result and evaluate best model on all metrics if all datasets are given\n",
    "    print(\"\\nBest MSE on Validation set: %.4g\"%best_value)\n",
    "    print(\"At training step %d\"%best_epoch)\n",
    "    if not path is None:\n",
    "        # saves last model and loads model from best epoch\n",
    "        model.save(path+'savemodels/model_last.h5')\n",
    "        model=load_model(path+'savemodels/model_best.h5')\n",
    "        if full_eval_bool:\n",
    "            best_model_result,pred_val,pred_nest=full_eval(model,x,x_val,y,y_val,in_nest.values,in_base.values, \n",
    "                                                           res_nest[result_param].values,res_base[result_param].values,\n",
    "                                                           results_scale[result_param],verbose=False)\n",
    "            best_model_result.to_csv(path+'bestmodel_result.csv')\n",
    "        \n",
    "    print(\"Training ended after %.1fs\"%(time.time()-start_t))\n",
    "    time_per_epoch=(time.time()-start_t)/(epoch+1)\n",
    "    \n",
    "    # if the model could not be evaluated on the other sets return a dummy\n",
    "    if not full_eval_bool:\n",
    "        best_model_result=None\n",
    "        \n",
    "    return trainCourse,model,time_per_epoch,best_model_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the predictions are evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(y,y_pred,results_scale):\n",
    "\n",
    "    # mkld needs the scaled values between 0 and 1 \n",
    "    res_max=results_scale.loc['max']\n",
    "    res_min=results_scale.loc['min']\n",
    "    \n",
    "    y=y*(res_max-res_min)+res_min\n",
    "    y_pred=y_pred*(res_max-res_min)+res_min\n",
    "\n",
    "    mse=np_mse(y,y_pred)\n",
    "    mae=np_mae(y,y_pred)\n",
    "    me=np_me(y,y_pred)\n",
    "    perc_mse_val=perc_mse(y,y_pred)\n",
    "    perc_mae_val=perc_mae(y,y_pred)\n",
    "    perc_me_val=perc_me(y,y_pred)\n",
    "\n",
    "    print(\"MSE:\\t\\t%.5g\"%mse)\n",
    "    print(\"MAE:\\t\\t%.5g\"%mae)\n",
    "    print(\"ME:\\t\\t%.5g\"%me)\n",
    "    print(\"Perc ME:\\t%.5g\"%perc_me_val)\n",
    "    \n",
    "    result={'mse':mse,'mae':mae,'me':me,'perc_mse':perc_mse_val,'perc_mae':perc_mae_val,'perc_me':perc_me_val}\n",
    "\n",
    "    return result\n",
    "\n",
    "        \n",
    "def full_eval(model,in_train,in_val,res_train,res_val,in_nest,in_base,res_nest, res_base,results_scale,verbose=False):\n",
    "    '''\n",
    "    Evaluates the model on all parameters and returns pandas dataframe with the results.\n",
    "    '''\n",
    "\n",
    "    # save results in dataframe\n",
    "    result=pd.DataFrame()\n",
    "    \n",
    "    y_pred_train=np.squeeze(model.predict(in_train))\n",
    "    print(\"\\nTraining Set:\")\n",
    "    res_train=evaluate_predictions(res_train,y_pred_train,results_scale)\n",
    "    for key in res_train:\n",
    "        result.loc[0,'train_'+key]=res_train[key]\n",
    "        \n",
    "    y_pred_val=np.squeeze(model.predict(in_val)) \n",
    "    print(\"\\nValidation Set:\")     \n",
    "    res_val=evaluate_predictions(res_val,y_pred_val,results_scale)\n",
    "    for key in res_val:\n",
    "        result.loc[0,'val_'+key]=res_val[key]\n",
    "\n",
    "    y_pred_nest=np.squeeze(model.predict(in_nest)) \n",
    "    print(\"\\nNested Set:\")\n",
    "    res_nest=evaluate_predictions(res_nest,y_pred_nest,results_scale)\n",
    "    for key in res_nest:\n",
    "        result.loc[0,'nest_'+key]=res_nest[key]\n",
    "    \n",
    "    y_pred_base=model.predict(in_base)\n",
    "    print(\"\\nBase Scenario:\")\n",
    "    res_base=evaluate_predictions(res_base,y_pred_base,results_scale)\n",
    "    for key in res_base:\n",
    "        result.loc[0,'base_'+key]=res_base[key]\n",
    "\n",
    "    # rescale data for overview file\n",
    "    y_pred_val_scaled_back=scale_back(y_pred_val, results_scale)\n",
    "    y_pred_nest_scaled_back=scale_back(y_pred_nest, results_scale)\n",
    "\n",
    "    return result,y_pred_val_scaled_back,y_pred_nest_scaled_back    \n",
    "\n",
    "\n",
    "def np_mse(y_true,y_pred):\n",
    "    '''\n",
    "    Returns the mean squared error for 2 numpy arrays.\n",
    "    '''\n",
    "    mse=0.\n",
    "    \n",
    "    for i in range(len(y_true)):\n",
    "        mse=mse+(y_true[i]-y_pred[i])**2\n",
    "    return mse/len(y_true)\n",
    "\n",
    "def np_mae(y_true,y_pred):\n",
    "    '''\n",
    "    Returns the mean absolute error for 2 numpy arrays.\n",
    "    '''\n",
    "    ae=abs(y_pred-y_true)\n",
    "    return ae.mean()\n",
    "\n",
    "def np_me(y_true,y_pred):\n",
    "    '''\n",
    "    Returns the mean error for 2 numpy arrays.\n",
    "    '''\n",
    "    ae=y_pred-y_true\n",
    "    return ae.mean()\n",
    "\n",
    "def perc_me(y_true,y_pred):\n",
    "    '''\n",
    "    Percentage form of the absolute error. \n",
    "    '''\n",
    "    p_me=(y_pred-y_true)/y_true\n",
    "    return p_me.mean()*100.\n",
    "\n",
    "def perc_mae(y_true,y_pred):\n",
    "    '''\n",
    "    Percentage form of the mean absolute error. \n",
    "    '''\n",
    "    ae=abs(y_pred-y_true)/y_true\n",
    "    return ae.mean()*100.\n",
    "\n",
    "def perc_mse(y_true,y_pred):\n",
    "    '''\n",
    "    Percentage form of the mean squared error. \n",
    "    '''\n",
    "    mse=0.\n",
    "    \n",
    "    for i in range(len(y_true)):\n",
    "        mse=mse+((y_true[i]-y_pred[i])/y_true[i])**2\n",
    "    return 100.*mse/len(y_true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the model is evaluated on given data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model,train_inputs,train_results,val_inputs,val_results,nest_inputs,nest_results,base_inputs,\n",
    "                      base_results,inputs_scale,results_scale):\n",
    "\n",
    "    output='o1'\n",
    "    result,pred_val,pred_nest=full_eval(model,train_inputs.values,\n",
    "                                     val_inputs.values,\n",
    "                                     train_results[output].values,\n",
    "                                     val_results[output].values,\n",
    "                                     nest_inputs.values,\n",
    "                                     base_inputs.values,\n",
    "                                     nest_results[output].values, \n",
    "                                     base_results[output].values,\n",
    "                                     results_scale[output],\n",
    "                                     verbose=True)\n",
    "            \n",
    "    return result,pred_val,pred_nest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the setup file and run the training with the hyperparameters from the setup file and perform the training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import keras.backend as K\n",
    "\n",
    "def run_training(save_path,setup_path,data_path,early_stopping=True,early_stopping_no=200):\n",
    "    \n",
    "    '''\n",
    "    save_path [string]  Path for saving the results.\n",
    "    \n",
    "    setup_path [string] Path to setup_NN.csv (contains hyperparameters).\n",
    "    \n",
    "    data_path [string]  Path to folder containing the data.   \n",
    "                \n",
    "    early_stopping [boolean] default true \n",
    "                        True if you want to stop early.\n",
    "                \n",
    "    early_stopping_no [int] default 200 \n",
    "                        Number of epochs after which you want to stop if no improvement.       \n",
    "                 \n",
    "    '''\n",
    "    \n",
    "    # assert save_path is a string and the path does not already exist\n",
    "    if not save_path is None:\n",
    "        assert type(save_path)==str, \"save_path has to be a string\"\n",
    "        if not save_path[-1]=='/':\n",
    "            save_path=save_path+'/'\n",
    "    \n",
    "    if os.path.exists(save_path):\n",
    "        if os.path.exists(save_path+'overview.csv'):\n",
    "            overview=pd.read_csv(save_path+'overview.csv',index_col=0)\n",
    "            assert not os.path.exists(save_path+'Model0'),\"Incomplete Training run in folder. Delete \"+save_path+'Model0'\n",
    "    else:\n",
    "        os.mkdir(save_path)\n",
    "        \n",
    "    # assert data_path is a string and the required files do exist\n",
    "    assert type(data_path)==str, \"data_path has to be a string\"\n",
    "    if not data_path[-1]=='/':\n",
    "        data_path=data_path+'/'\n",
    "    \n",
    "    # assert setup file exists\n",
    "    assert type(setup_path)==str, \"setup_path needs to be a string\"\n",
    "    assert os.path.exists(setup_path), \"Setup \"+setup_path+\" does NOT exist\"\n",
    "          \n",
    "    setup=pd.read_csv(setup_path)\n",
    "        \n",
    "    # convert strings from .csv file to list\n",
    "    for column in setup.columns:\n",
    "        setup.loc[0,column]=ast.literal_eval(setup.loc[0,column])\n",
    "    \n",
    "    # set time budget per evaluation in s\n",
    "    timebudget_per_eval=60*60\n",
    "    \n",
    "    # save the setup/searchspace\n",
    "    if not save_path is None:\n",
    "        setup.to_csv(save_path+'current_setup.csv',index=False)\n",
    "        \n",
    "    # read from setup file\n",
    "    batchsize = (setup.loc[0,'batchsize'])[0]\n",
    "    no_layers = (setup.loc[0,'no_layers'])[0]\n",
    "    dropouts = setup.loc[0,'dropouts']\n",
    "    no_nodes = setup.loc[0,'no_nodes']\n",
    "    activation = setup.loc[0,'activation']\n",
    "    initializer = setup.loc[0,'initializer']\n",
    "    optimizer = setup.loc[0,'optimizer']\n",
    "    lr = (setup.loc[0,'learning_rate'])[0]\n",
    "    output_activation = setup.loc[0,'output_activation_function']\n",
    "        \n",
    "    # get data\n",
    "    x,y,x_val,y_val,in_nest,res_nest,in_base,res_base,inputs_scale,results_scale=get_data(data_path)\n",
    "    \n",
    "    \n",
    "    input_dim=x.shape[1]\n",
    "    # set random seed otherwise everytime it is reset the same seed is used \n",
    "    # because numpy gets seeded in data.get_train_data\n",
    "    \n",
    "    # make overview file\n",
    "    overview=pd.DataFrame(columns=['no_layers','no_nodes','dropouts','batchsize','activation','initializer',\n",
    "                                    'optimizer','timebudget_per_eval','no_epochs','time_per_epoch','train_mse',\n",
    "                                    'val_mse','lr','early_stopping','early_stopping_no','output_activation_function'])\n",
    "       \n",
    "    start_t=time.time()\n",
    "        \n",
    "    model_name='Model0'\n",
    "    if not save_path is None:\n",
    "        save_path_model=save_path+model_name\n",
    "            \n",
    "    if not lr is None:\n",
    "        if optimizer=='RMSprop':\n",
    "            optimizer=kOpt.RMSprop(lr=lr)\n",
    "        elif optimizer=='adam':\n",
    "            optimizer=kOpt.Adam(lr=lr)\n",
    "                    \n",
    "        overview.loc[0,'no_layers']=no_layers\n",
    "        overview.at[0,'no_nodes']=no_nodes\n",
    "        overview.at[0,'dropouts']=dropouts\n",
    "        overview.loc[0,'batchsize']=batchsize\n",
    "        overview.loc[0,'activation']=activation\n",
    "        overview.loc[0,'initializer']=initializer\n",
    "        overview.loc[0,'optimizer']=type(optimizer).__name__\n",
    "        overview.loc[0,'timebudget_per_eval']=timebudget_per_eval\n",
    "        overview.loc[0,'lr']=lr\n",
    "        overview.loc[0,'early_stopping']=early_stopping\n",
    "        overview.loc[0,'early_stopping_no']=early_stopping_no\n",
    "        overview.loc[0,'output_activation_function']=output_activation\n",
    "            \n",
    "        model=setup_model(hidden_layers=no_nodes,dropouts=dropouts,activation=activation,input_dim=input_dim,\n",
    "                                initializer=initializer,optimizer=optimizer,output_activation=output_activation)\n",
    "    \n",
    "        trainCourse,model,time_epoch,best_res=train_model(model,x,y,x_val,\n",
    "                                                   y_val,epochs=2000,\n",
    "                                                   results_scale=results_scale,\n",
    "                                                   batchsize=batchsize,\n",
    "                                                   early_stopping = early_stopping,\n",
    "                                                   early_stopping_no=early_stopping_no,\n",
    "                                                   path=save_path_model,\n",
    "                                                   time_budget=timebudget_per_eval,\n",
    "                                                   in_nest=in_nest,\n",
    "                                                   in_hval=x_val,\n",
    "                                                   in_base=in_base,\n",
    "                                                   res_nest=res_nest,\n",
    "                                                   res_hval=y_val,\n",
    "                                                   res_base=res_base,\n",
    "                                                   val_metric='mae')\n",
    "            \n",
    "        # find best epoch\n",
    "        for column in best_res.columns:\n",
    "            overview.loc[0,column]=best_res.loc[0,column]\n",
    "                \n",
    "        overview.loc[0,'no_epochs'] = len(trainCourse['mse'])\n",
    "        overview.loc[0,'time_per_epoch'] = time_epoch\n",
    "    \n",
    "        if not save_path is None:\n",
    "            overview.to_csv(save_path+'/overview.csv')\n",
    "                \n",
    "        print(\"\\n\\nTotal Time: %.1fs\"%(time.time()-start_t))\n",
    "            \n",
    "    return overview,model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we perform a regression using a given polynomial term structure. A more sophisticated methods using an adaptive stepwise approach have been implemented in the industry, see Krah, Nikolić, Korn (2018).\n",
    "\n",
    "We read in the data without scaling it. Please note that the unscaled data are read in once again here. This was done for the sake of convenience and it allows a separate run of polynomial regression without a training of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_unscaled(path):\n",
    "    \n",
    "    # assert path is a string and the required files do exist\n",
    "    assert type(path)==str, \"path has to be a string\"\n",
    "    if not path[-1]=='/':\n",
    "        path=path+'/'\n",
    "    \n",
    "    assert os.path.exists(path+'train_input.csv'), \"File '\"+ path+\"train_input.csv' is missing\"\n",
    "    assert os.path.exists(path+'validation_input.csv'), \"File '\"+ path+\"validation_input.csv' is missing\"\n",
    "    assert os.path.exists(path+'nested_input.csv'), \"File '\"+ path+\"nested_input.csv' is missing\"\n",
    "    assert os.path.exists(path+'base_input.csv'), \"File '\"+ path+\"base_input.csv' is missing\"\n",
    "    assert os.path.exists(path+'train_result.csv'), \"File '\"+ path+\"train_result.csv' is missing\"\n",
    "    assert os.path.exists(path+'validation_result.csv'), \"File '\"+ path+\"validation_result.csv' is missing\"\n",
    "    assert os.path.exists(path+'nested_result.csv'), \"File '\"+ path+\"nested_result.csv' is missing\"\n",
    "    assert os.path.exists(path+'base_result.csv'), \"File '\"+ path+\"base_result.csv' is missing\"\n",
    "    \n",
    "        \n",
    "    # =============================================================================\n",
    "    #     Specify paths\n",
    "    # =============================================================================\n",
    "        \n",
    "    # train paths\n",
    "    path_train_res=path+'train_result.csv'\n",
    "    path_train_input=path+'train_input.csv'\n",
    "             \n",
    "    # validation paths  \n",
    "    path_val_res=path+'validation_result.csv'\n",
    "    path_val_input=path+'validation_input.csv'\n",
    "        \n",
    "    # nested paths\n",
    "    path_nest_res=path+'nested_result.csv'\n",
    "    path_nest_input=path+'nested_input.csv'\n",
    "        \n",
    "    # base paths\n",
    "    path_base_res=path+'base_result.csv'\n",
    "    path_base_input=path+'base_input.csv'\n",
    "\n",
    "    \n",
    "    # =============================================================================\n",
    "    #   load data\n",
    "    # =============================================================================\n",
    "        \n",
    "    # train set\n",
    "    train_results=pd.read_csv(path_train_res,index_col=0)\n",
    "    train_inputs=pd.read_csv(path_train_input,index_col=0)\n",
    "    \n",
    "    # validation set\n",
    "    val_results=pd.read_csv(path_val_res,index_col=0)\n",
    "    val_inputs=pd.read_csv(path_val_input,index_col=0)\n",
    "    \n",
    "    # nested set\n",
    "    nest_results=pd.read_csv(path_nest_res,index_col=0)\n",
    "    nest_inputs=pd.read_csv(path_nest_input,index_col=0)\n",
    "    \n",
    "    # base set\n",
    "    base_results=pd.read_csv(path_base_res,index_col=0)\n",
    "    base_results=base_results.iloc[[0],:]\n",
    "    base_inputs=pd.read_csv(path_base_input,index_col=0)\n",
    "   \n",
    "   \n",
    "    return train_inputs, train_results, val_inputs, val_results, nest_inputs, nest_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert a candidate vector to the corresponding data column. The terminology \"candidate\" comes from a wrapper which iteratively builds up a model using an information criterion. Here we provide a stripped up code as a minimum viable regression algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def candidate_to_vector(c, X):\n",
    "    \n",
    "    vec = 1\n",
    "    c_str = ''\n",
    "    \n",
    "    for i in range(X.shape[1]):\n",
    "        vec = vec * X[:,i]**c[i]                           \n",
    "        if c[i]>0:\n",
    "            # Generates a String for the output\n",
    "            c_str = c_str + 'i' + str(i+1)                   \n",
    "            if c[i]>1:\n",
    "                c_str = c_str + '^' + str(int(c[i]))\n",
    "            c_str = c_str + ' '        \n",
    "            \n",
    "    return (np.transpose(np.array([vec])), c_str)          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the least-squares regression. Setup file which contains the polynomial that is used in the regression is named 'setup_polynomial.csv'. We provide a version with just linear terms. Of course more complex models can easily be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from numpy import genfromtxt\n",
    "\n",
    "def export_results(data_path,result_param='o1'):\n",
    "   \n",
    "    # load data\n",
    "    train_inputs, train_results, scenarios_t, results_t,scenarios_nest,results_nest =get_data_unscaled(data_path)\n",
    "    \n",
    "    # load polynomial \n",
    "    model =  genfromtxt('./setups/setup_polynomial.csv', delimiter=',')\n",
    "    model = model[1:model.shape[0],1:model.shape[1]]\n",
    "\n",
    "    # ================================= #\n",
    "    # Training                          #\n",
    "    # ================================= #\n",
    "    \n",
    "    # calculate X for validation data\n",
    "    results= train_results[result_param]\n",
    "    inputs = np.ones([train_inputs.shape[0],1]).astype(float)\n",
    "    for i in (range(model.shape[0]-1)):\n",
    "        v, v_text= candidate_to_vector(model[i+1,:],train_inputs.values)\n",
    "        inputs = np.float64(np.append(arr=inputs, values=v, axis=1))\n",
    "        \n",
    "    # calculate regression coefficients\n",
    "    reg = LinearRegression().fit(inputs, results)    \n",
    "    \n",
    "    \n",
    "    # ================================== #\n",
    "    #   Validation Data                  #\n",
    "    # ================================== #\n",
    "\n",
    "    inputs_val = np.ones([scenarios_t.shape[0],1]).astype(float)\n",
    "    for i in (range(model.shape[0]-1)):\n",
    "        v, v_text = candidate_to_vector(model[i+1,:],scenarios_t.values)\n",
    "        inputs_val = np.float64(np.append(arr=inputs_val, values=v, axis=1))\n",
    "    # calculated y_pred for validation\n",
    "    y_pred_t = np.float64(reg.predict(inputs_val))\n",
    "        \n",
    "    # ================================= #\n",
    "    #   Nested data                     #\n",
    "    # ================================= #\n",
    "    \n",
    "    ## calculate X for nested data\n",
    "    inputs_nest = np.ones([scenarios_nest.shape[0],1]).astype(float)\n",
    "    for i in (range(model.shape[0]-1)):\n",
    "        v, v_text = candidate_to_vector(model[i+1,:],scenarios_nest.values)\n",
    "        inputs_nest = np.float64(np.append(arr=inputs_nest, values=v, axis=1))\n",
    "    \n",
    "    ## calculate regression coefficients   \n",
    "    #reg = LinearRegression().fit(X_nest, result_nest)\n",
    "    # calculated y_pred for nested data\n",
    "    y_pred_nest = np.float64(reg.predict(inputs_nest))\n",
    "\n",
    "    return(y_pred_t, y_pred_nest, model, reg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write overview file with inputs, output \"o1\" and predictions from the neural network and the polynomial for validation and nested data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_overview(data_path,save_path):\n",
    "    \n",
    "    # =============================================================================\n",
    "    #     Specify paths\n",
    "    # =============================================================================\n",
    "                 \n",
    "    # validation set\n",
    "    path_val_input=data_path+'validation_input.csv'\n",
    "    path_val_res=data_path+'validation_result.csv'\n",
    "    \n",
    "    # nested set\n",
    "    path_nest_input=data_path+'nested_input.csv'\n",
    "    path_nest_res=data_path+'nested_result.csv'\n",
    "    \n",
    "    # =============================================================================\n",
    "    #   load data\n",
    "    # =============================================================================\n",
    "\n",
    "    # validation set\n",
    "    val_results=pd.read_csv(path_val_res,index_col=0)\n",
    "    val_inputs=pd.read_csv(path_val_input,index_col=0)\n",
    "        \n",
    "    # nested set\n",
    "    nest_results=pd.read_csv(path_nest_res,index_col=0)\n",
    "    nest_inputs=pd.read_csv(path_nest_input,index_col=0)\n",
    "\n",
    "    \n",
    "    # =============================================================================\n",
    "    # write file\n",
    "    # =============================================================================\n",
    "\n",
    "    dim_val=val_inputs.shape[0]\n",
    "    inputs=val_inputs.shape[1]\n",
    "    dim_nest=nest_inputs.shape[0]\n",
    "    \n",
    "    # validation data\n",
    "    validation=pd.DataFrame(columns=val_inputs.columns)\n",
    "    \n",
    "    for i in range(inputs):\n",
    "        for j in range(dim_val):\n",
    "            validation.loc[j+1,'i'+str(i+1)] = val_inputs.loc[j+1,'i'+str(i+1)]\n",
    "    \n",
    "    validation[''] = ''\n",
    "    validation['o1'] = val_results['o1']\n",
    "\n",
    "    # prediction from neural network\n",
    "    path_pred_val=save_path+'/Neural_Network/prediction_val_nn.csv'\n",
    "    prediction_val=pd.read_csv(path_pred_val,index_col=0)\n",
    "    prediction_val.index = prediction_val.index + 1\n",
    "    validation.insert(inputs+2, 'Prediction NN ',prediction_val, True) \n",
    "    \n",
    "    # prediction from polynomial\n",
    "    prediction_val_poly = pd.read_csv(save_path+'/Polynomial/prediction_val_poly.csv',index_col=0)\n",
    "    prediction_val_poly.index = prediction_val_poly.index + 1\n",
    "    validation.insert(inputs+3, 'Polynomial', prediction_val_poly)    \n",
    "    \n",
    "    validation.to_csv(save_path+'/overview_validation.csv') \n",
    "\n",
    "\n",
    "    # nested data\n",
    "    nested=pd.DataFrame(columns=nest_inputs.columns)\n",
    "    \n",
    "    for i in range(inputs):\n",
    "        for j in range(dim_nest):\n",
    "            nested.loc[j+1,'i'+str(i+1)] = nest_inputs.loc[j+1,'i'+str(i+1)]\n",
    "            \n",
    "    nested[''] = ''\n",
    "    nested['o1'] = nest_results['o1']\n",
    "    \n",
    "    # prediction from neural network\n",
    "    path_pred_nest=save_path+'/Neural_Network/prediction_nest_nn.csv'\n",
    "    prediction_nest=pd.read_csv(path_pred_nest,index_col=0)\n",
    "    prediction_nest.index = prediction_nest.index + 1\n",
    "    nested.insert(inputs+2, 'Prediction NN ',prediction_nest, True) \n",
    "  \n",
    "    # prediction from polynomial\n",
    "    prediction_nest_poly = pd.read_csv(save_path+'/Polynomial/prediction_nest_poly.csv',index_col=0)\n",
    "    prediction_nest_poly.index = prediction_nest_poly.index + 1\n",
    "    nested.insert(inputs+3,'Polynomial', prediction_nest_poly, True)  \n",
    "\n",
    "    nested.to_csv(save_path+'/overview_nested.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the Neural Network and the Polynomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we actually run the code and save the predictions in the overview file.\n",
    "\n",
    "In order to run the code one needs to choose portfolio 1, 2 or 3. The results will be saved in './results/', e.g. save_path = './results/Portfolio_1_TIMESTAMP'.\n",
    "\n",
    "Input data will be read for the correct company, e.g. data_path = './Portfolio1/'.\n",
    "\n",
    "Setup file for neural network contains hyperparameters, it has to be named 'setup_NN.csv'. Parameters to be set in setup_NN.csv:\n",
    "- batchsize\n",
    "- no_layers\n",
    "- dropouts\n",
    "- no_nodes\n",
    "- activation\n",
    "- initializer\n",
    "- optimizer\n",
    "- learning_rate\n",
    "- output_activation_function\n",
    "\n",
    "Setup file for polynomial contains the polynomial term structure that is used in regression, it has to be named 'setup_polynomial.csv'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 13)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                700       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 70)                3570      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 70)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 50)                3550      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 7,871\n",
      "Trainable params: 7,871\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "32768 Training samples\n",
      "256 Validation samples\n",
      "\n",
      "==================Epoch 0==================\n",
      "Train MSE:\t\t0.25438\n",
      "Val MSE:\t\t0.028448\n",
      "Time: 6.67s\n",
      "Full Time: 6.7s\n",
      "Current best Epoch:   0\n",
      "\n",
      "==================Epoch 1==================\n",
      "Train MSE:\t\t0.046497\n",
      "Val MSE:\t\t0.0079448\n",
      "Time: 3.27s\n",
      "Full Time: 9.9s\n",
      "Current best Epoch:   1\n",
      "\n",
      "==================Epoch 2==================\n",
      "Train MSE:\t\t0.038249\n",
      "Val MSE:\t\t0.006059\n",
      "Time: 2.52s\n",
      "Full Time: 12.5s\n",
      "Current best Epoch:   2\n",
      "\n",
      "==================Epoch 3==================\n",
      "Train MSE:\t\t0.032125\n",
      "Val MSE:\t\t0.0021717\n",
      "Time: 2.95s\n",
      "Full Time: 15.4s\n",
      "Current best Epoch:   3\n",
      "\n",
      "==================Epoch 4==================\n",
      "Train MSE:\t\t0.030172\n",
      "Val MSE:\t\t0.0045167\n",
      "Time: 1.49s\n",
      "Full Time: 16.9s\n",
      "Current best Epoch:   3\n",
      "\n",
      "==================Epoch 5==================\n",
      "Train MSE:\t\t0.029173\n",
      "Val MSE:\t\t0.0024391\n",
      "Time: 1.40s\n",
      "Full Time: 18.3s\n",
      "Current best Epoch:   3\n",
      "\n",
      "==================Epoch 6==================\n",
      "Train MSE:\t\t0.028729\n",
      "Val MSE:\t\t0.0021345\n",
      "Time: 2.52s\n",
      "Full Time: 20.8s\n",
      "Current best Epoch:   6\n",
      "\n",
      "==================Epoch 7==================\n",
      "Train MSE:\t\t0.028078\n",
      "Val MSE:\t\t0.0018377\n",
      "Time: 2.52s\n",
      "Full Time: 23.3s\n",
      "Current best Epoch:   7\n",
      "\n",
      "==================Epoch 8==================\n",
      "Train MSE:\t\t0.027556\n",
      "Val MSE:\t\t0.002271\n",
      "Time: 2.32s\n",
      "Full Time: 25.7s\n",
      "Current best Epoch:   7\n",
      "\n",
      "==================Epoch 9==================\n",
      "Train MSE:\t\t0.027315\n",
      "Val MSE:\t\t0.0012341\n",
      "Time: 3.21s\n",
      "Full Time: 28.9s\n",
      "Current best Epoch:   9\n",
      "\n",
      "==================Epoch 10==================\n",
      "Train MSE:\t\t0.027334\n",
      "Val MSE:\t\t0.0019993\n",
      "Time: 1.91s\n",
      "Full Time: 30.8s\n",
      "Current best Epoch:   9\n",
      "\n",
      "==================Epoch 11==================\n",
      "Train MSE:\t\t0.026344\n",
      "Val MSE:\t\t0.0023794\n",
      "Time: 1.47s\n",
      "Full Time: 32.3s\n",
      "Current best Epoch:   9\n",
      "\n",
      "==================Epoch 12==================\n",
      "Train MSE:\t\t0.026525\n",
      "Val MSE:\t\t0.0027766\n",
      "Time: 1.34s\n",
      "Full Time: 33.6s\n",
      "Current best Epoch:   9\n",
      "\n",
      "==================Epoch 13==================\n",
      "Train MSE:\t\t0.026274\n",
      "Val MSE:\t\t0.0024699\n",
      "Time: 2.82s\n",
      "Full Time: 36.4s\n",
      "Current best Epoch:   9\n",
      "\n",
      "==================Epoch 14==================\n",
      "Train MSE:\t\t0.025731\n",
      "Val MSE:\t\t0.0015404\n",
      "\n",
      "Model has not improved for 5 consecutive Epochs\n",
      "Early Stopping...\n",
      "\n",
      "Best MSE on Validation set: 0.02738\n",
      "At training step 9\n",
      "\n",
      "Training Set:\n",
      "MSE:\t\t0.014572\n",
      "MAE:\t\t0.080402\n",
      "ME:\t\t0.011469\n",
      "Perc ME:\t1.1186\n",
      "\n",
      "Validation Set:\n",
      "MSE:\t\t0.0012341\n",
      "MAE:\t\t0.027383\n",
      "ME:\t\t0.0085696\n",
      "Perc ME:\t0.21662\n",
      "\n",
      "Nested Set:\n",
      "MSE:\t\t0.0013895\n",
      "MAE:\t\t0.03284\n",
      "ME:\t\t-0.023223\n",
      "Perc ME:\t-3.3201\n",
      "\n",
      "Base Scenario:\n",
      "MSE:\t\t0.00073381\n",
      "MAE:\t\t0.027089\n",
      "ME:\t\t-0.027089\n",
      "Perc ME:\t-2.7089\n",
      "Training ended after 48.1s\n",
      "\n",
      "\n",
      "Total Time: 49.2s\n",
      "\n",
      "Training Set:\n",
      "MSE:\t\t0.014572\n",
      "MAE:\t\t0.080402\n",
      "ME:\t\t0.011469\n",
      "Perc ME:\t1.1186\n",
      "\n",
      "Validation Set:\n",
      "MSE:\t\t0.0012341\n",
      "MAE:\t\t0.027383\n",
      "ME:\t\t0.0085696\n",
      "Perc ME:\t0.21662\n",
      "\n",
      "Nested Set:\n",
      "MSE:\t\t0.0013895\n",
      "MAE:\t\t0.03284\n",
      "ME:\t\t-0.023223\n",
      "Perc ME:\t-3.3201\n",
      "\n",
      "Base Scenario:\n",
      "MSE:\t\t0.00073381\n",
      "MAE:\t\t0.027089\n",
      "ME:\t\t-0.027089\n",
      "Perc ME:\t-2.7089\n",
      "\n",
      "Overview files can be found in ./results/Portfolio_1_2020_Aug_15_20-14-57\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import xlsxwriter\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# INPUT: Portfolio number #\n",
    "portfolio = '1'           \n",
    "###########################\n",
    "\n",
    "data_path = './Portfolio'+portfolio+'/'\n",
    "\n",
    "today=datetime.now()\n",
    "timestamp=today.strftime(\"%Y_%b_%d_%H-%M-%S\")\n",
    "\n",
    "save_path = './results/Portfolio_'+portfolio+'_'+timestamp\n",
    "\n",
    "assert not os.path.exists(save_path), \"Folder '\"+ save_path +\"' already exists\"\n",
    "os.makedirs(save_path)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# NEURAL NETWORK\n",
    "# =============================================================================\n",
    "\n",
    "setup_path_NN = './setups/setup_NN.csv'\n",
    "save_path_NN = save_path+'/Neural_Network'\n",
    "\n",
    "'''\n",
    "set (if desired to be different from default)\n",
    "- early_stopping (default = True)\n",
    "- early_stopping_no (default = 200)\n",
    "'''\n",
    "early_stopping=True\n",
    "early_stopping_no=5\n",
    "\n",
    "# Run neural network training\n",
    "overview,model=run_training(save_path_NN,setup_path_NN,data_path,early_stopping=early_stopping,\n",
    "                            early_stopping_no=early_stopping_no)\n",
    "\n",
    "# Load data\n",
    "train_inputs,train_results,val_inputs,val_results,nest_inputs,nest_results,base_inputs,base_results,inputs_scale,results_scale=get_data(data_path)\n",
    "\n",
    "# Evaluate model\n",
    "result,pred_val_NN,pred_nest_NN=evaluate(model,train_inputs,train_results,val_inputs,val_results,\n",
    "                      nest_inputs,nest_results,base_inputs,base_results,inputs_scale,results_scale)\n",
    "result.to_csv(save_path_NN+'/results.csv')\n",
    "\n",
    "# Save predictions\n",
    "prediction_val_NN=pd.DataFrame({'pred_val':pred_val_NN})\n",
    "prediction_val_NN.to_csv(save_path_NN+'/prediction_val_nn.csv') \n",
    "\n",
    "prediction_nest_NN=pd.DataFrame({'pred_nest':pred_nest_NN})\n",
    "prediction_nest_NN.to_csv(save_path_NN+'/prediction_nest_nn.csv') \n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Polynomial\n",
    "# =============================================================================\n",
    "\n",
    "save_path_poly = save_path+'/Polynomial/'\n",
    "os.mkdir(save_path_poly)\n",
    "\n",
    "# run regression and get results\n",
    "pred_val_poly,pred_nest_poly, model, coef = export_results(data_path)\n",
    "\n",
    "prediction_val_poly=pd.DataFrame({'pred_val':pred_val_poly})\n",
    "prediction_val_poly.to_csv(save_path_poly+'/prediction_val_poly.csv') \n",
    "\n",
    "prediction_nest_poly=pd.DataFrame({'pred_nest':pred_nest_poly})\n",
    "prediction_nest_poly.to_csv(save_path_poly+'/prediction_nest_poly.csv') \n",
    "\n",
    "xlsx_name = 'Polynomial_Coefficients.xlsx'\n",
    "\n",
    "workbook = xlsxwriter.Workbook(save_path_poly + xlsx_name)\n",
    "worksheet = workbook.add_worksheet(\"Coefficients\")\n",
    "row = 1\n",
    "col = 1\n",
    "for j in range(model.shape[1]):\n",
    "    worksheet.write(0, col+j, 'i'+str(j+1))\n",
    "for i in range(model.shape[0]):\n",
    "    for j in range(model.shape[1]):\n",
    "        worksheet.write(i+1, j+1, model[i][j])\n",
    "for i in range(len(coef)):\n",
    "    worksheet.write(i+1,0,coef[i])\n",
    "worksheet.write(0,0,'Coefficient')\n",
    "workbook.close()\n",
    "\n",
    "\n",
    "\n",
    "# ================================================================================================\n",
    "#  Write overview file with inputs, output o1 and prediction from neural network and polynomial\n",
    "# ================================================================================================\n",
    "\n",
    "write_overview(data_path,save_path)\n",
    "print('\\nOverview files can be found in',save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
